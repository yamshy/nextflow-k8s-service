"""Parser for demo workflow results."""

from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional

from ..models import DemoResultMetrics

logger = logging.getLogger(__name__)


def parse_report_json(
    report_path: str,
    execution_time_seconds: float,
    worker_pods_spawned: int,
) -> Optional[DemoResultMetrics]:
    """Parse the report.json file generated by the demo workflow.

    Args:
        report_path: Path to the report.json file
        execution_time_seconds: Total execution time in seconds
        worker_pods_spawned: Number of worker pods spawned during execution

    Returns:
        DemoResultMetrics if parsing succeeds, None if file not found or invalid
    """
    try:
        report_file = Path(report_path)
        if not report_file.exists():
            logger.warning("Report file not found: %s", report_path)
            return None

        with report_file.open() as f:
            data = json.load(f)

        # Extract batch statistics
        batches = data.get("batches", [])
        total_batches = data.get("total_batches", len(batches))

        # Aggregate metrics across all batches
        total_records = sum(batch.get("lines", 0) for batch in batches)
        total_sum = sum(batch.get("sum", 0) for batch in batches)

        # Calculate average value across all records
        average_value = total_sum / total_records if total_records > 0 else 0.0

        return DemoResultMetrics(
            total_batches=total_batches,
            total_records=total_records,
            total_sum=total_sum,
            average_value=round(average_value, 2),
            worker_pods_spawned=worker_pods_spawned,
            execution_time_seconds=round(execution_time_seconds, 2),
            report_path=report_path,
        )

    except json.JSONDecodeError as exc:
        logger.exception("Failed to parse report.json: %s", exc)
        return None
    except Exception as exc:
        logger.exception("Error reading report file: %s", exc)
        return None


def extract_batch_metrics_from_logs(log_lines: list[str]) -> tuple[int, int]:
    """Extract batch progress metrics from pipeline logs.

    Parses Nextflow logs to count:
    - Number of GENERATE processes completed
    - Number of ANALYZE processes completed

    Args:
        log_lines: List of log lines from the pipeline

    Returns:
        Tuple of (batches_generated, batches_analyzed)
    """
    batches_generated = 0
    batches_analyzed = 0

    for line in log_lines:
        # Nextflow marks completed processes with specific patterns
        # Example: "[XX/XXXXXX] Submitted process > GENERATE (1)"
        # Example: "[XX/XXXXXX] process > ANALYZE (1) [100%] 1 of 1 ✔"
        if "process > GENERATE" in line and ("✔" in line or "Cached" in line):
            batches_generated += 1
        elif "process > ANALYZE" in line and ("✔" in line or "Cached" in line):
            batches_analyzed += 1

    return batches_generated, batches_analyzed


def estimate_completion_time(
    started_at: datetime,
    batches_generated: int,
    batches_analyzed: int,
    total_batches: int,
) -> Optional[datetime]:
    """Estimate completion time based on current progress.

    The demo workflow runs in 45-60 seconds typically. We estimate based on:
    - GENERATE processes (first half, ~20-30s)
    - ANALYZE processes (second half, ~15-20s)
    - REPORT process (final step, ~5-10s)

    Args:
        started_at: When the run started
        batches_generated: Number of GENERATE processes completed
        batches_analyzed: Number of ANALYZE processes completed
        total_batches: Total number of batches being processed

    Returns:
        Estimated completion datetime, or None if cannot estimate
    """
    if total_batches == 0:
        return None

    # Calculate progress percentage
    # GENERATE weight: 40%, ANALYZE weight: 40%, REPORT weight: 20%
    generate_progress = (batches_generated / total_batches) * 0.4
    analyze_progress = (batches_analyzed / total_batches) * 0.4
    total_progress = generate_progress + analyze_progress

    if total_progress == 0:
        # Just started, estimate 60 seconds
        from datetime import timedelta

        return started_at + timedelta(seconds=60)

    # Estimate total runtime based on current progress
    elapsed = (datetime.now() - started_at).total_seconds()
    if elapsed > 0 and total_progress > 0:
        estimated_total = elapsed / total_progress
        from datetime import timedelta

        return started_at + timedelta(seconds=estimated_total)

    return None
